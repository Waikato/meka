\documentclass[11pt]{article}
\usepackage[margin=1.1in]{geometry}
\usepackage{my_symbols}
\usepackage{hyperref}
\newcommand{\MEKA}{Meka}
\newcommand{\MICE}{Mice}
\newcommand{\MOA}{Moa}
\newcommand{\WEKA}{Weka}
\newcommand{\MULAN}{Mulan}
\def\version{{\tt 1.7.6}}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\small\ttfamily,breaklines=true,frame=L,xleftmargin=\parindent}
%\lstset{framextopmargin=50pt,frame=bottomline}
%\lstset{breaklines=true} 
%\lstset{breakatwhitespace=true} 

% if you just need a simple heading
% Usage:
%   \heading{the text of the heading}
\newcommand{\heading}[1]{
    \vspace{0.3cm} \noindent \textbf{#1} \newline
}

\usepackage{datetime}
\newdateformat{mydate}{\monthname[\THEMONTH] \THEYEAR}

\begin{document}

\title{Tutorial. \framework{\MEKA} \version }

\author{Jesse Read}

\date{\mydate\today}

\maketitle

\begin{center}
	\includegraphics{MEKA.png}\\
		A Multilabel/multitarget Extension to WEKA.\\
		\url{http://meka.sourceforge.net}
\end{center}

\tableofcontents

\thispagestyle{empty}
\pagebreak

\section{Introduction}

This is a tutorial for the open source machine learning framework \framework{\MEKA}. \framework{\MEKA} is closely based upon the \framework{\WEKA} framework \cite{WEKA}; providing support for development, running and evaluation of \keyword{multi-label} and \keyword{multi-target} classifiers (which \framework{WEKA} does not).

In the \keyword{multi-label} problem, a data instance may be associated with multiple labels. This is as opposed to the traditional task of single-label classification (\ie multi-class, or binary) where each instance is only associated with a single class label. The multi-label context is receiving increased attention and is applicable to a wide variety of domains, including text, music, images and video, and bioinformatics. A good introduction can be found in \cite{MMD} and \cite{Thesis}.

The multi-label problem is in fact a special case of \keyword{multi-target} learning. In multi-target, or \textit{multi-dimensional} learning, a data instance is associated with multiple target variables, where each variable takes a number of values. In the multi-label case, all variables are binary, indicating label relevance ($1$) or irrelevance ($0$). The multi-target case has been investigated by, for example, \cite{UPM} and \cite{MT}.  

\framework{\MEKA} can also includes \emph{incremental} classifiers suitable for the \keyword{data streams} context. An overview of some of the methods included in \framework{\MEKA} for learning from incremental data streams is given in \cite{MEDS2}.

\framework{\MEKA} is released under the GNU GPL v3 licence. The latest release, source code, API reference, this tutorial, and further information and links to additional material, can be found at the website: \url{http://meka.sourceforge.net}. 

This tutorial applies to \framework{\MEKA} version {\version}. 

\section{Getting Started}

\framework{\MEKA} can be download from: \url{http://sourceforge.net/projects/meka/files/}. This tutorial is written for version \version; and assumes that you have downloaded and extracted the \texttt{meka-release-\version} and that \texttt{meka-\version}, found within, is your current working directory.

\subsection{Requirements}

\framework{\MEKA} requires:

\begin{itemize}
	 \item Java version 1.6 or above
\end{itemize}

%\subsection{Downloading}
\framework{\MEKA} comes bundled with other packages such as \framework{\WEKA}'s \texttt{weka.jar}, and also \framework{\MULAN}'s \texttt{mulan.jar} for running classifiers from this framework. % If \framework{\WEKA} is already installed on your system, it must be at least version release (\texttt{3.7.X}) to be compatible with \texttt{\MEKA}.
These files are found in the \texttt{lib} directory. See \texttt{lib/README.txt} for version information.


%\framework{\MEKA}'s incremental classification extension (\texttt{mice.jar}) comes packaged in a separate jar file. 

\subsection{Running}
\label{sec:running}

\framework{\MEKA} can be used very easily from the command line. For example, to run the Binary Relevance (BR) classifier on the \textit{Music} dataset; type:
\begin{lstlisting}
java -cp "./lib/*" meka.classifiers.multilabel.BR -t data/Music.arff
\end{lstlisting}

If you are on a Microsoft Windows system, you need to use back slashes instead of forward slashes (\texttt{.\textbackslash lib\textbackslash*}). If you add the \texttt{jar} files to the system's \texttt{CLASSPATH}, you do not need to supply the \texttt{-cp} option at runtime. For the remainder of examples in this tutorial we will assume that this is the case.

Since Version 1.2 \framework{\MEKA} has a graphical user interface (GUI). Run this with either the \texttt{run.sh} script (under Linux or OSX) as follows:
\begin{lstlisting}
./run.sh
\end{lstlisting}
Run \texttt{run.bat} instead if you are using Microsoft windows.


%\begin{lstlisting}
%	export CLASSPATH=$MEKA/meka.jar:$MEKA/weka.jar:$CLASSPATH
%\end{lstlisting}
%
%where \texttt{\$MEKA} is the folder you extracted the software to; or by supplying the classpath directly when running java with :


\section{\label{sec:format}\framework{\MEKA}'s Dataset Format}

\framework{\MEKA} uses \framework{\WEKA}'s ARFF file format. See \url{http://weka.wikispaces.com/ARFF} to learn about this format. \framework{\MEKA} uses multiple attributes -- one for each target or label -- rather than a single class attribute. The \emph{number} of target attributes is specified with either \texttt{-C} or \texttt{-c}; \emph{unlike} in \framework{\WEKA} where the \texttt{-c} flag indicates the position of the \emph{class index}. \framework{\MEKA} uses the reference to the \texttt{classIndex} internally to denote the number of target attributes. %This is important when creating new \framework{\MEKA} classifiers (see Section \ref{sec:extending}).

Since the number of target attributes tends to vary with each dataset, for convenience \framework{\MEKA} allows this option (as well as other dataset options like the train/test split percentage) to be stored in the \texttt{@relation} name of an ARFF file, where a colon (\texttt{:}) is used to separate the dataset name and the options. The following is an example ARFF header for multi-target classification with three target variables and four attributes:
 
{\small

\begin{lstlisting}
@relation 'Example_Dataset: -C 3'

@attribute category {A,B,C,NEG}
@attribute label {0,1}
@attribute rank {1,2,3}
@attribute X1 {0,1}
@attribute X2 {0,1}
@attribute X3 numeric
@attribute X4 numeric

@data
\end{lstlisting}
}

Note that the format of the \texttt{label} attribute (binary) is the \emph{only} kind of target attribute in multi-\emph{label} datasets. For more examples of \framework{\MEKA} ARFF files; see the \texttt{data/} directory for several multi-label and multi-target datasets (some of these are in a compressed format).

\framework{\MEKA} can also read ARFF files in the \framework{\MULAN} format where target attributes are the \emph{last} attributes, rather than the first ones. This format can also be read by \framework{\MEKA} by specifying a minus sign `\texttt{-}' before the number of target attributes in the \texttt{-C} option. For example, \texttt{-C -3} will set the \emph{last} \texttt{3} attributes as the target attributes automatically when the file is loaded. Alternatively, the target attributes can be moved using \framework{\WEKA}'s \texttt{Reorder} filter, or in the GUI as detailed in the following section. %weka.filters.unsupervised.attribute.

%Note that the \framework{\MULAN} format expects the target attributes to be indexed \emph{last} rather than the first. Specifying \texttt{-c $-L$} to \framework{\MEKA} will automatically move the last $L$ attributes to the beginning upon loading. 

\subsection{Manipulating Datasets in the GUI}
\label{sec:data.gui}

A good way to set up an ARFF file for multi-dimensional classification is using the GUI. Open an ARFF file with `\textsf{Open}' from the \textsf{File} menu. In the \textsf{Preprocess} tab in the right-hand column (see \Fig{screen:arff}; note that the class attributes are listed in \textbf{bold} face), simply select the attributes you wish to use as class attributes and click the button `\textsf{Use class attributes}'. You can then save this file using `\textsf{Save}' from the \textsf{File} menu, which will also save the \texttt{-C} flag into the \texttt{@relation} tag as described above (displayed under '\textsf{Relation:}' in the GUI), so next time the classes will be set automatically.

The datasets that come with \framework{\MEKA} already come with the \texttt{-C} flag specified correctly, so you do not need to set this information.

You can also run any of \framework{\WEKA}'s filters on the dataset with the \textsf{Choose} button. See the \framework{\WEKA} documentation for more information.

\begin{figure}
	\centering
	\includegraphics[height=0.75\textwidth]{GUI01.png}
	\caption{\label{screen:arff} \framework{MEKA}'s GUI interface; having loaded the \textit{Music} dataset.}
\end{figure}

\section{Using \framework{\MEKA}}%: Running Experiments

%With any suitable dataset it is possible t\framework{\MEKA} classifiers.
A suitable dataset is the only requirement to begin running experiments with \framework{\MEKA}.

\subsection{Command Line Interface}

With the exception of the different use of the \texttt{-c} flag (see the previous section), many of \framework{\WEKA}'s command line options for evaluation work identically in \framework{\MEKA} too. You can obtain a list of them by running any classifier with the \texttt{-h} flag, for example: \texttt{java meka.classifiers.multilabel.BR -h} displays the following:
	 
{\small
\begin{lstlisting}
Evaluation Options:

-h
        Output help information.
-t <name of training file>
        Sets training file.
-T <name of test file>
        Sets test file.
-x <number of folds>
        Do cross-validation with this many folds.
-R
        Randomise the dataset (done after a range is removed, but before the train/test split).
-split-percentage <percentage>
        Sets the percentage for the train/test set split, e.g., 66.
-split-number <number>
        Sets the number of training examples, e.g., 800
-i
        Invert the specified train/test split.
-s <random number seed>
        Sets random number seed (use with -R, for different CV or train/test splits).
-threshold <threshold>
        Sets the type of thresholding; where
                'PCut1' automatically calibrates a threshold (the default);
                'PCutL' automatically calibrates one threshold for each label;
                any number, e.g. '0.5', specifies that threshold.
-C <number of classes/labels>
        Sets the number of target attributes (classes/labels) to expect (indexed from the beginning).
-d <classifier_file>
        Specify a file to dump classifier into.
-l <classifier_file>
        Specify a file to load classifier from.
-verbosity <verbosity level>
        Specify more/less evaluation output


Classifier Options:

-W
        Full name of base classifier.
        (default: weka.classifiers.trees.J48)
-output-debug-info
	If set, classifier is run in debug mode and
	may output additional info to the console
--do-not-check-capabilities
	If set, classifier capabilities are not checked before classifier is built
	(use with caution).
\end{lstlisting}
}

The only required options are \texttt{-t} to specify the dataset, and \texttt{-C} to specify the number of target attributes (which is typically included within the dataset, as explained in the previous section).  

The \texttt{Classifier Options} are specific to each classifier, which in this case (for \texttt{java meka.classifiers.multilabel.BR}) are not very extensive. However, in nearly all methods, performance varies considerably with the \emph{base classifier}. In version 1.7.5 the default is \texttt{J48} (decision trees). The base classifier is changed with the \texttt{-W} option. For example, to use Naive Bayes, type\footnote{If typed on one line, the backslash `\texttt{\textbackslash}' should be omitted} on the command line:

\begin{lstlisting}
java meka.classifiers.multilabel.BR -t data/Music.arff \
  -W weka.classifiers.bayes.NaiveBayes
\end{lstlisting}

\subsection{Graphical User Interface}

The CLI is the most powerful way to work with \framework{\MEKA}, but the GUI is a good way to get started. Refer to \Sec{sec:running} on how to open the GUI. Once opened, you will see three tabs: \textsf{Preprocess}, \textsf{Classify}, \textsf{Visualize}. The following process will guide you through a simple experiment.

%Again, to run \framework{\MEKA}'s GUI, simply type \texttt{ant run} from the X directory, or run either the \texttt{run.sh} (under Linux) or \texttt{run.bat} (under Microsoft windows) scripts in the \texttt{scripts/} folder. 



\begin{enumerate}
	\item Load a dataset file using \textsf{Open} from the file menu. %See \Sec{sec:data.gui} for information on preprocessing the datasets.
	\item Click on the \textsf{Classify} tab.
	\item \textsf{Choose} a multi-label or multi-target classifier (or leave the default choice). 
	\item Click on the label to the right of this button and set specific options to the classifier. In most cases, these options also involve setting a \framework{\WEKA} single-label base classifier (and also its options). For \framework{\MEKA}'s meta classifiers, you will need to first choose a \framework{\MEKA} base classifier, and then a single-label \framework{\WEKA} base classifier for this classifier. See, for example, \Fig{screen:eval}, using a \texttt{BaggingML} ensemble of \texttt{CC} (classifier chains) with \texttt{SMO} as the single-label base classifier.
	\item In the \textsf{Evaluation} panel you configure what type of evaluation you want to do, and some of the options given in the previous section are available here. For example, a 55/45 train/test split, as being specified in \Fig{screen:split}.
	\item When you click \textsf{Start} the experiment will be run. When finished, the result will appear in the \textsf{History} panel. This is the same output as would be seen on the command line, and explained in the following section. The verbosity of the results is controlled by the \textsf{Verbosity} option.
	%\item You can export results to text, or copy of paste it.
	\item Optionally, you can click on the \textsf{Visualize} tab and visualize the results.
\end{enumerate}




\begin{figure}
	\centering
	\includegraphics[height=0.60\textwidth]{GUI02.png}
	\caption{\label{screen:eval} \framework{MEKA}'s GUI interface; setting Bagging of Classifier Chains with SMO.}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[height=0.60\textwidth]{GUI03.png}
	\caption{\label{screen:split} \framework{MEKA}'s GUI interface; setting a train/test split.}
\end{figure}

\subsection{\label{sec:evaluation}Evaluation}

Running a BR classifier with Naive Bayes on the \textit{Music} data will output the following:

{\small
\begin{lstlisting}
Classifier_name      : meka.classifiers.multilabel.BR
Classifier_ops       : [-W, weka.classifiers.bayes.NaiveBayes]
Classifier_info      : 
Dataset_name         : Music
Type                 : ML
Threshold            : 0.9974578524138343
Verbosity            : 1

N(test)              : 237
L                    : 6    
Accuracy             : 0.436
Hamming score        : 0.745
Exact match          : 0.215

N_train              : 355
N_test               : 237
LCard_train          : 1.789
LCard_test           : 1.992
Build_time           : 0.157
Test_time            : 0.113
Total_time           : 0.27 
\end{lstlisting}
}

The results of the experiment in \Fig{screen:split} are shown in \Fig{screen:result}.
\begin{figure}
	\centering
	\includegraphics[height=0.95\textwidth]{GUI04.png}
	\caption{\label{screen:result} \framework{MEKA}'s GUI interface; results.}
\end{figure}

Note that by increasing the verbosity level, you can get more or less output. On the command line, this is with \texttt{-verbosity <n>} where \texttt{<n>} = 
\begin{center}
\begin{tabular}{ll}
	\hline
	\texttt{verbosity}       & Output \\
	\hline
	\texttt{1} (default) & basic output \\
	 \texttt{2}           & plus more metrics \\
	  \texttt{3}           & plus individual metrics (for each label) \\
	   \texttt{4}           & plus more individual metrics (for each label) \\
		\texttt{5}           & plus individual classifications \\
	  \texttt{6}           & plus individual confidence outputs (rounded to 1 d.p.) \\
	   \texttt{7}           & \ldots (rounded to 2 d.p.) \\
		\texttt{8}           & \ldots (rounded to 3 d.p.) \\
	\hline
\end{tabular}
\end{center}

Most of these metrics are described in \cite{ECC2,MMD}. The most common measures reported in the multi-label literature are \texttt{Hamming Loss} -- the accuracy for each label, averaged across all labels; \texttt{Exact Match} -- the accuracy of each example (all label relevances must match exactly for an example to be correct); and \texttt{Accuracy} (called Jaccard Index in information retrieval), which is neither as strict as Exact Match nor as `easy' as Hamming Loss.

%\subsection{Thresholds}

Note that a \texttt{Threshold} has been calibrated automatically; to minimise the difference between the label cardinality of the training set and the predictions on the test set; a practice described in \cite{ECC2}. To calibrate a threshold for \emph{each} label, add the \texttt{-threshold PCutL} option\footnote{Note: this was \texttt{-T C} in previous versions of \framework{\MEKA}}. This gives a vector of thresholds which, in this case, increases \texttt{Accuracy} slightly (to \texttt{0.456}). Different thresholds are calculated for different methods and datasets. Note that \texttt{PCutL} is being specified in \Fig{screen:split}, which results in a threshold for each label in \Fig{screen:result}.

%\begin{lstlisting}
%          Threshold : [0.29971988795518206, 0.26330532212885155, \
%			  0.4257703081232493, 0.22128851540616246, 0.23809523809523808, \
%			  0.3473389355742297]
%\end{lstlisting}

\framework{\MEKA} also supports \emph{cross validation}; for example:
\begin{lstlisting}
java meka.classifiers.multilabel.BR -x 10 -R -t data/Music.arff \
  -W weka.classifiers.bayes.NaiveBayes
\end{lstlisting}
conducts 10 fold cross validation on a randomised version of the \textit{Music.arff} data and outputs the average results across all folds with standard deviation. %Note that using the \texttt{-f} option in this case to save output to a file will output only the results of the validation; which can be useful for compiling results later.

%Sometimes it can be useful to analyse the actual predictions made at test time (for example if you wish to use an evaluation metric not included in \framework{\MEKA}). \framework{\MEKA} can produce plain-text files with the option \texttt{-f <file name>}; for example:
%\begin{lstlisting}
%java meka.classifiers.multilabel.BR -t data/Music.arff \ 
%  -f BR-NB.meka \
%  -W weka.classifiers.bayes.NaiveBayes
%\end{lstlisting}
%which produces the plain-text file \texttt{BR-NB.meka}. This provides a way to do further evaluation `offline'. Note that when cross validation (\texttt{-x}) is used, %one file will be created for each fold, e.g., \texttt{BR-NB.meka.0}, \ldots, \texttt{BR-NB.meka.4} for a 5-fold validation. The results can be recalculated for any of %these files with, e.g.:
%\begin{lstlisting}
%	java weka.core.Result -f BR-NB.meka
%\end{lstlisting}
%Or, if you used cross validation, with:
%\begin{lstlisting}
%	java weka.core.Result -f BR-NB.meka -x 5
%\end{lstlisting}
%In this way, it is also possible to use \framework{\MEKA}'s evaluation procedures with output from other software (simply by using the right text output format).

\subsection{Examples} 

In the following we give some examples, with an emphasis on general usage and parameters. An extensive list of examples is given at \url{http://meka.sourceforge.net/methods.html}. 

%\paragraph{Multi-label}

%The methods used in the following examples are detailed in papers papers detailing these methods are available here.

\paragraph{Binary Relevance} (BR) On the \textit{Music} data, loading from two separate sets, using Naive Bayes as a base classifier, calibrating a separate threshold automatically for each label: 
\begin{lstlisting}
java meka.classifiers.multilabel.BR \
  -t data/Music_train.arff \
  -T data/Music_test.arff \
  -threshold PCutL \
  -W weka.classifiers.bayes.NaiveBayes
\end{lstlisting}

\paragraph{Ensembles of Pruned Sets} (EPS; see \cite{EPS}) With 10 ensemble members (the default) on the \textit{Enron} dataset with Support Vector Machines as the base classifier; each \texttt{PS} model is set with \texttt{N=1} and \texttt{P} to a random selection of \texttt{\{1,2,3,4,5\}}:

\begin{lstlisting}
java meka.classifiers.multilabel.meta.EnsembleML \
  -t data/Yeast.arff \
  -W meka.classifiers.multilabel.PS -- \
    -P 1-5 -N 1 -W weka.classifiers.functions.SMO
\end{lstlisting}

\paragraph{Ensembles of Classifier Chains} (ECC; see \cite{ECC2}) With 50 ensemble members (\texttt{-I 50}), and some textual output (\texttt{-output-debug-info}) on the \textit{Enron} dataset with Support Vector Machines as a base classifier:
\begin{lstlisting}
java meka.classifiers.multilabel.meta.BaggingML -I 50 -P 100 \
  -output-debug-info -t data/Enron.arff -W meka.classifiers.multilabel.CC\ 
  -- -W weka.classifiers.functions.SMO
\end{lstlisting}

\paragraph{Mulan Classifier} (\texttt{RAkEL} see \cite{RAKEL}) With parameters \texttt{\texttt{k=3}, \texttt{m=2C}} where \texttt{C} is the number of labels (these options are hardwired; you need to edit \texttt{MULAN.java} to specify new parameter configurations) on the \textit{Scene} dataset with Decision Trees as the base classifier (remember {\texttt{mulan.jar} must be in the classpath}):
\begin{lstlisting}
java meka.classifiers.multilabel.MULAN -t data/Scene.arff -verbosity 5 \ 
  -S RAkEL2 -W weka.classifiers.trees.J48
\end{lstlisting}
the \texttt{-verbosity 5} options increases the amount of evaluation output.

%\paragraph{Cross Validation} the following carries out cross validation on ECC using Logistic Regression as a base classifier, on the \textit{Music} data.

%\begin{lstlisting}
%java meka.classifiers.multilabel.meta.BaggingML \
%  -x 10 -R -t data/Music.arff \
%  -I 10 -W meka.classifiers.multilabel.CC -- \
%  -W weka.classifiers.functions.Logistic
%\end{lstlisting}

\paragraph{Incremental Classification: Ensembles of Binary Relevance} (see \cite{ECC2,MEDS2}) With 10 ensemble members (default) on the \textit{Enron} dataset with \texttt{NaiveBayesUpdateable} as a base classifier; using over 20 evaluation windows:

\begin{lstlisting}
java meka.classifiers.multilabel.incremental.meta.BaggingMLUpdateable\
  -B 20 -t data/Enron.arff \
  -W meka.classifiers.multilabel.incremental.BRUpdateable -- \
    -W weka.classifiers.bayes.NaiveBayesUpdateable
\end{lstlisting}

Evaluating incremental classifiers will carry out evaluation for the data over $\texttt{B}-1$ evaluation windows (an initial window is used for initial training but is not included in evaluation). The result shown is the average across all windows. On the command line, the per-window evaluation statistics are also shown, with a high-enough \texttt{verbosity}. %Note that the \framework{\MOA} framework \cite{MOA} for evaluating incremental classifiers is much more sophisticated; and a new release will include a wrapper to \framework{\MEKA}. 

\paragraph{Multi-target: Ensembles of Class Relevance} (see \cite{UPM}) The multi-target version of the Binary Relevance classifier) on the \textit{solar flare} dataset with Logistic Regression as a base classifier under $10$-fold cross-validation:
\begin{lstlisting}
java meka.classifiers.multitarget.meta.BaggingMT -x 10 -R \
  -t data/solar_flare.arff \
  -W meka.classifiers.multitarget.CR -- \
    -W weka.classifiers.functions.Logistic
\end{lstlisting}
 
\paragraph{Semi-supervised: EM} with CC with Logistic Regression as a base classifier, using a separate set of unlabelled data (in two separate commands):
\begin{lstlisting}
java meka.classifiers.multilabel.meta.EM -t data-train.arff \
  -T data-unlabelled.arff -d em.saved
java meka.classifiers.multilabel.meta.EM -t data-train.arff \
  -T data-test.arff -l em.saved
\end{lstlisting}
%\begin{lstlisting}
%java -cp meka.jar:weka.jar weka.classifiers.multitarget.meta.BaggingMT -x 5 -R -t data/solar_flare.arff -W weka.%classifiers.multitarget.C    C -- -W weka.classifiers.functions.Logistic
%\end{lstlisting}

\section{Development}
\label{development}
The following sections explain a bit more in detail of how to obtain MEKA's source code, how to compile it and how to develop new algorithms.

\subsection{Source code}
\label{sourcecode}
For obtaining the source code of MEKA, you have two options:
\begin{itemize}
  \item Using subversion\footnote{\url{http://subversion.apache.org/}}
  \item Release archive
\end{itemize}

\noindent In the case of \textit{subversion}, you can obtain the source code using the following command in the console (or \textit{command prompt} for Windows users):
\begin{lstlisting}
svn checkout svn://svn.code.sf.net/p/meka/code/trunk meka
\end{lstlisting}
This will create a new directory called \texttt{meka} in the current directory, containing the source code and build scripts.

Instead of using subversion, you can simply use the source code that is part of each MEKA release, contained in the \texttt{meka-src-\version.jar} Java archive\footnote{\url{http://en.wikipedia.org/wiki/JAR\_(file\_format)}}. A Java archive can be opened with any archive manager that can handle ZIP files.

\subsection{Compiling}
\label{compiling}
\heading{Using \texttt{ant}}
MEKA uses \textit{Apache ant}\footnote{\url{http://ant.apache.org/}} as build tool. You can compile MEKA as follows:
\begin{lstlisting}
ant clean compile jar
\end{lstlisting}
If you want to generate an archive containing all source code, pdfs and binary jars, then you can use the following command:
\begin{lstlisting}
ant clean compile release
\end{lstlisting}
Please note, if you develop new algorithms, you should also create a unit test for it, to ensure that it is working properly. See section \ref{unittests} for more details.

\heading{Using Eclipse}
After obtaining the source code, you can use the Eclipse\footnote{\url{http://eclipse.org/}} template files that come with MEKA. You only need to rename two files, found in the same directory as the \texttt{build.xml} ant build script, as follows:
\begin{itemize}
  \item \texttt{.classpath.default} $\rightarrow$ \texttt{.classpath}
  \item \texttt{.project.default} $\rightarrow$ \texttt{.project}
\end{itemize}
Then you can simply import MEKA as \textit{Existing Java Project}, pointing the import wizard to the directory containing the above mentioned files.

\subsection{Unit tests}
\label{unittests}
MEKA uses the JUnit 3.8.x unit testing framework.

\noindent A classifier test case is derived from the following abstract super class:
\begin{lstlisting}
meka.classifiers.AbstractMekaClassifierTest
\end{lstlisting}

\noindent A filter test case is derived from the following abstract super class:
\begin{lstlisting}
meka.filters.AbstractMekaFilterTest
\end{lstlisting}

\noindent You can execute all the unit tests by calling the following class from the command-line:
\begin{lstlisting}
meka.MekaTests
\end{lstlisting}

\subsection{\label{sec:extending}Extending \framework{\MEKA}}%: Writing New Classifiers

Writing \framework{\MEKA} classifiers involves writing regular \framework{\WEKA} classifiers that extend either the \texttt{MultilabelClassifier} or \texttt{MultiTargetClassifier} class, and expect the \texttt{classIndex()} of \texttt{Instance}s and \texttt{Instances}s to indicate the number of target attributes (indexed at the beginning) rather than the class index (as explained in Section \ref{sec:format}). 

The following is an example of a functioning (but extremely minimalistic) classifier, \texttt{TestClassifier}, that predicts $0$-relevance for all labels:

{\small
\lstset{basicstyle=\small\ttfamily,breaklines=true,language=java,frame=L,xleftmargin=\parindent}
\begin{lstlisting}
package meka.classifiers.multilabel;
import weka.core.*;

public class TestClassifier extends MultilabelClassifier {
	
    public void buildClassifier(Instances D) throws Exception {
        testCapabilities(D);
        int C = D.classIndex();
    }
    
    public double[] distributionForInstance(Instance x) throws Exception {
        int C = x.classIndex();
       	return new double[C];
    }
    
    public static void main(String args[]) {
        MultilabelClassifier.runClassifier(new TestClassifier(),args);
    }
}
\end{lstlisting}
}

This shows how easy it is to create a new classifier. However, for more useful examples see the source code of existing \framework{\MEKA} classifiers. The \texttt{testCapabilities(D)} line is optional but highly recommended. Note that the \texttt{distributionForInstance} method returns a \texttt{double[]} array exactly like in \framework{\WEKA}. However, whereas in \framework{\WEKA}, there is one value in the array for each possible value of the single target attribute, in \framework{\MEKA} this function returns an array of $C$ values, where $C$ is the \emph{number} of target attributes, and the $j$th value of the array is the \emph{value} corresponding to the $j$th target attribute.

\subsubsection{Building Classifiers}

In the \texttt{buildClassifier(Instances)} method, you build your classifier. Here is where you can take advantage of all of \framework{WEKA}'s libraries. You can use any \WEKA classifier to your needs. The \texttt{m\_Classifier} variable is already available for this, which \textit{already contains} the \framework{WEKA} classifier you specify on the command line. You can simply do:
{
\lstset{basicstyle=\small\ttfamily,breaklines=true,language=java,frame=L,xleftmargin=\parindent}
\begin{lstlisting}
    public void buildClassifier(Instances D) throws Exception {
        testCapabilities(D);
        int C = D.classIndex();
        D.setClassIndex(0);
        m_Classifier.buildClassifier(D);
    }
\end{lstlisting}
}
to train a classifier to learn the first label of your data (using the other labels, and all other input-space feature attributes). So if you then run on the command line
{
\lstset{basicstyle=\small\ttfamily,breaklines=true,language=java,frame=L,xleftmargin=\parindent}
\begin{lstlisting}
java meka.classifiers.multilabel.TestClassifier -t data/Music.arff \
	-W weka.classifiers.functions.SMO
\end{lstlisting}
}
an instantiation of \texttt{SMO} will already be be available in \texttt{m\_Classifier}. You can also do this explicitly with
{
\lstset{basicstyle=\small\ttfamily,breaklines=true,language=java,frame=L,xleftmargin=\parindent}
\begin{lstlisting}
...
    m_Classifier = new SMO();
    m_Classifier.buildClassifier(D);
...
\end{lstlisting}
}

\subsubsection{Classifying New Instances}

In the multi-label case, for a test \texttt{Instance x}, the \texttt{double[]} array returned by the method \texttt{distributionForInstance(x)} might contain the $0/1$ label relevances, for example (assuming \texttt{-C 5}):
\begin{lstlisting}
	[0.0, 0.0, 1.0, 1.0, 0.0]
\end{lstlisting} 
or it might contain posterior probabilities / prediction confidences / votes for each label, for example:
\begin{lstlisting}
	[0.1, 0.0, 0.9, 0.9, 0.2]
\end{lstlisting} 
where clearly the third and fourth labels are most relevant. Under a threshold of $0.5$ the final classification for \texttt{x} would be \texttt{[0,0,1,1,0]}. \framework{\MEKA} will by default automatically calibrate a threshold to convert all values into $0/1$ relevances like these (see Section \ref{sec:evaluation}). 

%\subsubsection{Multi-target Classifiers}

In the multi-target case, the \texttt{double[]} values returned by the method \texttt{distributionForInstance} must indicate the \emph{relevant class value}; for example (assuming \texttt{-C 3}): 
\begin{lstlisting}
	[3.0, 1.0, 0.0]
\end{lstlisting} 
If this were the dataset exemplified in \ref{sec:format}, this classification would be \texttt{C}, \texttt{1}, and \texttt{1} for the class attributes \texttt{category}, \texttt{label}, and \texttt{rank}, respectively.

Note that no threshold is calibrated. However, any associated voting or probabilistic values may be stored in the following $\texttt{C+1},\ldots,\texttt{2C}$ values; for example (again assuming \texttt{-C 6}):
\begin{lstlisting}
	[3.0, 1.0, 0.0, 0.5, 0.9, 0.9]
\end{lstlisting} 
where \texttt{C} is predicted as the value of the first target attribute with confidence $0.9$, and so on. However these values are currently only for use at classification time (for example the voting scheme of an ensemble method, see \texttt{weka.classifiers.multitarget.BaggingMT}); and not taken into account for evaluation. Note also that we intend to deprecate this method in the future, so avoid if possible.

\subsubsection{Incremental Classifiers}

\framework{\MEKA} comes with incremental versions of many classifiers as well as incremental evaluation methods, located in the \texttt{meka/classifiers/multilabel/incremental/} folder. Incremental classifiers implement \framework{\WEKA}'s \texttt{UpdatebleClassifier} interface and therefore must implement the \texttt{updateClassifier(Instance)} method. The following extends \texttt{TestClassifier} for incremental learning.

{
\small
\lstset{basicstyle=\small\ttfamily,breaklines=true,language=java,frame=L,xleftmargin=\parindent}
\begin{lstlisting}
package meka.classifiers.multilabel.incremental;
import meka.classifiers.multilabel.TestClassifier;
import weka.classifiers.UpdateableClassifier;
import weka.core.*;

public class TestClassifierUpdateable extends TestClassifier 
    implements UpdateableClassifier{
	
    public void updateClassifier(Instance x) throws Exception {
        int L = x.classIndex();
    }
    
    public static void main(String args[]) {
        IncrementalEvaluation.runExperiment(
            new TestClassifierUpdateable(),args);
    }
}
\end{lstlisting}
}

Note that the \texttt{IncrementalEvaluation} class is called for evaluation in this case; see Section \ref{sec:evaluation}. For analysis of data streams, the \framework{\MOA} framework \cite{MOA} contains more possibilities than \framework{\WEKA}. For this reason we include the \texttt{moa.jar} in \framework{\MEKA}, and can be used via \framework{WEKA}'s \texttt{MOA} meta classifier (wrapper), e.g., 
\begin{lstlisting}
java meka.classifiers.multilabel.BRUpdateable -t data/Music.arff \
	-W weka.classifiers.meta.MOA -- -B moa.classifiers.trees.HoeffdingTree
\end{lstlisting}

Note that \framework{\MOA} now also supports \framework{\MEKA} classifiers via a wrapper class; and is currently being developed to support a range of incremental multi-label classification and evaluation for data streams.

\subsubsection{Semi-supervised Classifiers}

Semi-supervised classifiers implement \texttt{SemisupervisedClassifier} interface and must implement the method \texttt{setUnlabelledData(Instances)} which is called automatically by the \texttt{Evaluation} class prior to \texttt{buildClassifier(Instances)}. It is up to the classifier to decide what to do with the unlabelled instances. Currently, \framework{\MEKA} assumes that unlabelled instances are the test instances (e.g., supplied with the \texttt{-T} flag). It sets all labels to missing beforehand just in case. In the case of a separate set of unlabelled instances, a model can be built, given unlabelled instances with \texttt{-T}, and then saved/dumped into a file with the \texttt{-d <filename>} option, and loaded again (with the \texttt{-l} option) along with a \emph{different} set with \texttt{-T}. 

\section{Getting Help / Reporting Bugs / Contributing}

A list of methods available using \framework{\MEKA} is maintained at \url{http://meka.sourceforge.net/methods.html} with descriptions and command-line examples.

If you need help with \framework{\MEKA}, you can post your problem on \MEKA's \textsf{Mailing List}: \url{http://sourceforge.net/p/meka/mailman/}.

If you have found a bug with \framework{\MEKA}, you can report in via the \textsf{Tracker}  of \framework{\MEKA}'s \url{SourceForge.net} site: \url{http://sourceforge.net/p/meka/bugs/}.

If you would like to contribute to \framework{\MEKA}, such as adding new classifiers, please get in touch with the developers.

More information (such as contact information) can be found at the \framework{\MEKA} website: \url{http://meka.sourceforge.net}.

\include{bibliography}

\end{document}
